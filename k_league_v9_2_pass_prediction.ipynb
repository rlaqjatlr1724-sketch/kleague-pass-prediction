{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlBef4kTYw7u"
      },
      "source": [
        "# ğŸ† K-League Pass Prediction - v9.2 Score Diff & Feature Clean\n",
        "\n",
        "## ë³€ê²½ì‚¬í•­ (v9.1 â†’ v9.2)\n",
        "- **LSTM ì…ë ¥ì— ì ìˆ˜ì°¨ ì¶”ê°€**: ì‹¤ì‹œê°„ ì ìˆ˜ì°¨ í”¼ì²˜ (9D â†’ 10D)\n",
        "- **ë¶ˆí•„ìš” í”¼ì²˜ ì œê±°**:\n",
        "  - `total_distance` ì œê±° (mean_distanceì™€ ì¤‘ë³µì„±)\n",
        "  - `is_weekend` ì œê±° (ì˜í–¥ ë¯¸ë¯¸)\n",
        "\n",
        "## LSTM ì…ë ¥ (10D)\n",
        "```\n",
        "[dx, dy, dist, angle, time, dist_to_goal, angle_to_goal, dist_to_own_goal, dist_to_center, score_diff]\n",
        "```\n",
        "\n",
        "## ì ìˆ˜ì°¨ ê³„ì‚° ë°©ì‹\n",
        "- ì—í”¼ì†Œë“œ ë‚´ Goal ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸\n",
        "- ìš°ë¦¬íŒ€ ê³¨ - ìƒëŒ€íŒ€ ê³¨\n",
        "- ì •ê·œí™”: / 5.0 (í° ì ìˆ˜ì°¨ ë“œë¬¾)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part1 ì‚¬ì „ì¤€ë¹„"
      ],
      "metadata": {
        "id": "0pcYXj0Gh0lC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocAGkSyXYw7w"
      },
      "source": [
        "!pip install pandas numpy scikit-learn tqdm torch lightgbm -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF1HfTxOYw7x",
        "outputId": "831a78c5-dcc5-4f23-eee2-952bfcd7d08f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy_wwMicYw7y",
        "outputId": "3a1e9fff-ab10-4596-c1b5-73571b14c575"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import lightgbm as lgb\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-_r1yd4Yw7y"
      },
      "source": [
        "## 02. í•˜ì´í¼íŒŒë¼ë¯¸í„°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQAX4T1hYw7y",
        "outputId": "491f0d86-9661-4aa7-d8a3-c9b9b42b2fbb"
      },
      "source": [
        "# Seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ê²½ë¡œ\n",
        "BASE_DIR = \"/content/drive/MyDrive/ColabNotebooks/K_league/data_open_track1\"\n",
        "TRAIN_PATH = f\"{BASE_DIR}/train.csv\"\n",
        "TEST_PATH = f\"{BASE_DIR}/test.csv\"\n",
        "MATCH_INFO_PATH = f\"{BASE_DIR}/match_info.csv\"\n",
        "\n",
        "# LSTM ì„¤ì •\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "LSTM_NUM_LAYERS = 2\n",
        "LSTM_DROPOUT = 0.3\n",
        "EMBEDDING_DIM = 16\n",
        "BATCH_SIZE = 64\n",
        "LSTM_EPOCHS = 100\n",
        "LR = 0.001\n",
        "\n",
        "# LGBM ì„¤ì •\n",
        "LGBM_PARAMS = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'max_depth': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 127,\n",
        "    'learning_rate': 0.03,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_samples': 20,\n",
        "    'verbose': -1,\n",
        "    'n_estimators': 2000,\n",
        "    'random_state': SEED\n",
        "}\n",
        "\n",
        "print(\"âœ… ì„¤ì • ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2HoizqsYw7z"
      },
      "source": [
        "---\n",
        "## 03. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF9KZQbbYw7z",
        "outputId": "654aec71-35b0-4b9a-ab1c-c32bee6c30ba"
      },
      "source": [
        "print(\"ğŸ“‚ ë°ì´í„° ë¡œë“œ...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "match_info = pd.read_csv(MATCH_INFO_PATH)\n",
        "\n",
        "# ì •ë ¬\n",
        "train_df = train_df.sort_values(['game_episode', 'time_seconds']).reset_index(drop=True)\n",
        "\n",
        "print(f\"âœ… Train: {len(train_df):,}í–‰\")\n",
        "print(f\"âœ… Match Info: {len(match_info)}ê²½ê¸°\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ ë°ì´í„° ë¡œë“œ...\n",
            "âœ… Train: 356,721í–‰\n",
            "âœ… Match Info: 228ê²½ê¸°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EusM0esWYw7z"
      },
      "source": [
        "### 3.1 Yì¶• Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFR1fJ40Yw7z",
        "outputId": "3dfb7034-fd8d-4e6a-ac6b-7a6d4f4280b8"
      },
      "source": [
        "def augment_y_flip_sequences(sequences):\n",
        "    \"\"\"\n",
        "    ì‹œí€€ìŠ¤ ë ˆë²¨ì—ì„œ Yì¶• ëŒ€ì¹­ Augmentation\n",
        "    Train ë°ì´í„°ì—ë§Œ ì ìš© (Val ëˆ„ìˆ˜ ë°©ì§€)\n",
        "    \"\"\"\n",
        "    augmented = []\n",
        "    for s in sequences:\n",
        "        aug = {\n",
        "            'continuous': s['continuous'].copy(),\n",
        "            'types': s['types'].copy(),\n",
        "            'results': s['results'].copy(),\n",
        "            'target': s['target'].copy(),\n",
        "            'last_abs_coords': s['last_abs_coords'].copy(),\n",
        "            'episode_id': str(s['episode_id']) + '_aug',\n",
        "            'meta_features': s['meta_features'].copy()\n",
        "        }\n",
        "        # Yì¶• ë°˜ì „\n",
        "        aug['continuous'][:, 1] *= -1  # dy ë°˜ì „\n",
        "        aug['continuous'][:, 3] *= -1  # angle ë°˜ì „\n",
        "        aug['target'][1] *= -1  # target dy ë°˜ì „\n",
        "        aug['last_abs_coords'][1] = 1.0 - aug['last_abs_coords'][1]\n",
        "        aug['meta_features']['last_y'] = 1.0 - aug['meta_features']['last_y']\n",
        "        augmented.append(aug)\n",
        "\n",
        "    return sequences + augmented\n",
        "\n",
        "print(\"âœ… Yì¶• ëŒ€ì¹­ ì¦ê°• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (Train/Val ë¶„í•  í›„ ì ìš©)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Yì¶• ëŒ€ì¹­ ì¦ê°• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (Train/Val ë¶„í•  í›„ ì ìš©)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd7NfONWYw7z"
      },
      "source": [
        "### 3.2 Global í”¼ì²˜ ì¤€ë¹„ (Player, Match Context)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXLxV0nAYw7z",
        "outputId": "ee01c9e6-a10e-48f4-f117-20adb052a75d"
      },
      "source": [
        "# Type/Result ë§¤í•‘\n",
        "type_map = {name: i for i, name in enumerate(train_df['type_name'].unique())}\n",
        "result_map = {name: i for i, name in enumerate(train_df['result_name'].fillna(\"Unknown\").unique())}\n",
        "NUM_TYPES, NUM_RESULTS = len(type_map), len(result_map)\n",
        "\n",
        "print(f\"Types: {NUM_TYPES}, Results: {NUM_RESULTS}\")\n",
        "\n",
        "# Player Role ì¶”ì • (KMeans)\n",
        "def create_player_roles(df):\n",
        "    stats = df.groupby('player_id').agg({\n",
        "        'start_x': 'mean',\n",
        "        'type_name': lambda x: list(x)\n",
        "    }).reset_index()\n",
        "\n",
        "    stats['is_gk'] = stats['type_name'].apply(\n",
        "        lambda x: any(t in ['Catch', 'Parry', 'Goal Keeping'] for t in x)\n",
        "    )\n",
        "    stats['role'] = 0  # GK\n",
        "\n",
        "    field = stats[~stats['is_gk']].copy()\n",
        "    if len(field) > 3:\n",
        "        km = KMeans(n_clusters=3, random_state=SEED, n_init=10).fit(field[['start_x']])\n",
        "        idx = np.argsort(km.cluster_centers_.flatten())\n",
        "        cmap = {old: new + 1 for new, old in enumerate(idx)}  # 1=DF, 2=MF, 3=FW\n",
        "        field['role'] = pd.Series(km.labels_, index=field.index).map(cmap)\n",
        "        stats.loc[field.index, 'role'] = field['role']\n",
        "\n",
        "    return dict(zip(stats['player_id'], stats['role']))\n",
        "\n",
        "role_map = create_player_roles(train_df)\n",
        "print(f\"âœ… Player Roles: {len(role_map)}ëª…\")\n",
        "\n",
        "# Player í†µê³„\n",
        "orig_train = train_df.copy()\n",
        "orig_train['step_dx'] = orig_train['end_x'] - orig_train['start_x']\n",
        "orig_train['step_dy'] = orig_train['end_y'] - orig_train['start_y']\n",
        "orig_train['step_dist'] = np.sqrt(orig_train['step_dx']**2 + orig_train['step_dy']**2)\n",
        "\n",
        "player_stats = orig_train.groupby('player_id').agg({\n",
        "    'step_dist': 'mean',\n",
        "    'step_dx': 'mean'\n",
        "}).reset_index()\n",
        "player_stats.columns = ['player_id', 'player_avg_dist', 'player_avg_dx']\n",
        "player_stats_map = player_stats.set_index('player_id').to_dict('index')\n",
        "\n",
        "print(f\"âœ… Player Stats: {len(player_stats_map)}ëª…\")\n",
        "\n",
        "# ========== íŒ€ í†µê³„ ì¶”ê°€ (NEW) ==========\n",
        "# íŒ€ë³„ í‰ê·  íŒ¨ìŠ¤ ê±°ë¦¬\n",
        "team_pass_stats = orig_train.groupby('team_id').agg({\n",
        "    'step_dist': 'mean'\n",
        "}).reset_index()\n",
        "team_pass_stats.columns = ['team_id', 'team_avg_pass_dist']\n",
        "\n",
        "# íŒ€ë³„ ì ìœ ìœ¨ (ì´ë²¤íŠ¸ ìˆ˜ ë¹„ìœ¨ë¡œ ê·¼ì‚¬)\n",
        "total_events = len(orig_train)\n",
        "team_possession = orig_train.groupby('team_id').size() / total_events\n",
        "team_possession = team_possession.reset_index()\n",
        "team_possession.columns = ['team_id', 'team_possession_ratio']\n",
        "\n",
        "# í•©ì¹˜ê¸°\n",
        "team_stats_df = team_pass_stats.merge(team_possession, on='team_id')\n",
        "team_stats_map = team_stats_df.set_index('team_id').to_dict('index')\n",
        "\n",
        "print(f\"âœ… Team Stats: {len(team_stats_map)}íŒ€\")\n",
        "print(f\"   í‰ê·  íŒ¨ìŠ¤ ê±°ë¦¬: {team_stats_df['team_avg_pass_dist'].mean():.2f}m\")\n",
        "print(f\"   ì ìœ ìœ¨ ë²”ìœ„: {team_stats_df['team_possession_ratio'].min():.3f} ~ {team_stats_df['team_possession_ratio'].max():.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Types: 26, Results: 9\n",
            "âœ… Player Roles: 446ëª…\n",
            "âœ… Player Stats: 446ëª…\n",
            "âœ… Team Stats: 12íŒ€\n",
            "   í‰ê·  íŒ¨ìŠ¤ ê±°ë¦¬: 12.55m\n",
            "   ì ìœ ìœ¨ ë²”ìœ„: 0.075 ~ 0.092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5INLnTz8Yw70",
        "outputId": "aa9d628b-f898-4ae0-992c-f05e9ac9af85"
      },
      "source": [
        "# Match Context ì²˜ë¦¬\n",
        "def process_match_info(matches):\n",
        "    df = matches.copy()\n",
        "    df['match_date_kst'] = pd.to_datetime(df['game_date']) + pd.Timedelta(hours=9)\n",
        "    df['match_hour'] = df['match_date_kst'].dt.hour\n",
        "    df['is_weekend'] = (df['match_date_kst'].dt.weekday >= 5).astype(int)\n",
        "\n",
        "    # Rest days ê³„ì‚°\n",
        "    home = df[['match_date_kst', 'home_team_id']].rename(columns={'home_team_id': 'team_id'})\n",
        "    away = df[['match_date_kst', 'away_team_id']].rename(columns={'away_team_id': 'team_id'})\n",
        "    full_schedule = pd.concat([home, away])\n",
        "    full_schedule['date_only'] = full_schedule['match_date_kst'].dt.normalize()\n",
        "    full_schedule = full_schedule.drop_duplicates().sort_values(['team_id', 'date_only'])\n",
        "    full_schedule['prev_date'] = full_schedule.groupby('team_id')['date_only'].shift(1)\n",
        "    full_schedule['rest_days'] = (full_schedule['date_only'] - full_schedule['prev_date']).dt.days\n",
        "    full_schedule['rest_days'] = full_schedule['rest_days'].fillna(7).clip(0, 14)\n",
        "\n",
        "    rest_map = dict(zip(\n",
        "        zip(full_schedule['team_id'], full_schedule['date_only'].dt.date.astype(str)),\n",
        "        full_schedule['rest_days']\n",
        "    ))\n",
        "    df['date_str'] = df['match_date_kst'].dt.date.astype(str)\n",
        "    df['home_rest'] = df.apply(lambda x: rest_map.get((x['home_team_id'], x['date_str']), 7), axis=1)\n",
        "    df['away_rest'] = df.apply(lambda x: rest_map.get((x['away_team_id'], x['date_str']), 7), axis=1)\n",
        "    df['rest_diff'] = df['home_rest'] - df['away_rest']\n",
        "    df['time_slot'] = pd.cut(df['match_hour'], bins=[-1, 17, 24], labels=[0, 1]).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "match_info_processed = process_match_info(match_info)\n",
        "match_context = match_info_processed[[\n",
        "    'game_id', 'match_hour', 'time_slot', 'is_weekend',\n",
        "    'home_rest', 'away_rest', 'rest_diff',\n",
        "    'home_team_id', 'away_team_id', 'home_score', 'away_score'\n",
        "]].copy()\n",
        "\n",
        "# íŒ€ í‰ê·  ë“ì \n",
        "home_gm = match_info.groupby('home_team_id')['home_score'].mean().to_dict()\n",
        "away_gm = match_info.groupby('away_team_id')['away_score'].mean().to_dict()\n",
        "\n",
        "print(f\"âœ… Match Context ì¤€ë¹„ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Match Context ì¤€ë¹„ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbebdUCqYw70"
      },
      "source": [
        "---\n",
        "## 04. í”¼ì²˜ ì •ì˜ ë° ê³„ì‚°\n",
        "\n",
        "LSTM , LGBM\n",
        "\n",
        "### í•µì‹¬: íë¦„ vs ìƒí™© ë¶„ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRMnBuxJYw70",
        "outputId": "820243b5-f0df-4fef-9302-146b43871ac7"
      },
      "source": [
        "\"\"\"\n",
        "í”¼ì²˜ ë¶„ë¥˜ ê¸°ì¤€:\n",
        "\n",
        "ğŸ”¹ LSTMìš© (ì‹œí€€ìŠ¤/íë¦„) - ë§¤ íƒ€ì„ìŠ¤í… ë³€í™”ê°€ ì¤‘ìš”\n",
        "   - dx, dy (ì´ë™ ë²¡í„°)\n",
        "   - distance, angle (ì´ë™ í¬ê¸°/ë°©í–¥)\n",
        "   - time delta\n",
        "   - dist_to_goal, angle_to_goal, dist_to_own_goal (ê³¨ëŒ€ ê´€ë ¨)\n",
        "   - dist_to_center (ì¤‘ì•™ì„  ê±°ë¦¬)\n",
        "   - score_diff (ì‹¤ì‹œê°„ ì ìˆ˜ì°¨) â† NEW\n",
        "\n",
        "ğŸ”¸ LGBMìš© (ìƒí™© ìš”ì•½) - í˜„ì¬ ìƒíƒœì˜ ìŠ¤ëƒ…ìƒ·\n",
        "   - ìœ„ì¹˜: dist_to_goal, angle_to_goal, zone_id\n",
        "   - ì¤‘ì•™ì„ : dist_to_center, is_in_own_half\n",
        "   - í†µê³„: mean_dist, forward_ratio, seq_length\n",
        "   - ì»¨í…ìŠ¤íŠ¸: role, player_stats, team_stats, match_phase\n",
        "   - Lag1: ì§ì „ ì´ë™ ì •ë³´\n",
        "\"\"\"\n",
        "\n",
        "# LSTM ì‹œí€€ìŠ¤ í”¼ì²˜ (ë§¤ ìŠ¤í…) - ì ìˆ˜ì°¨ ì¶”ê°€\n",
        "LSTM_CONTINUOUS_DIM = 10  # [dx, dy, dist, angle, time, dist_to_goal, angle_to_goal, dist_to_own_goal, dist_to_center, score_diff]\n",
        "\n",
        "# LGBM ë©”íƒ€ í”¼ì²˜ ëª©ë¡ (total_distance, is_weekend ì œê±°)\n",
        "LGBM_FEATURES = [\n",
        "    # ìœ„ì¹˜ ìƒíƒœ\n",
        "    'last_x', 'last_y',\n",
        "\n",
        "    # Zone\n",
        "    'zone_id',\n",
        "\n",
        "    # ê³¨ëŒ€ ê´€ë ¨\n",
        "    'dist_to_goal', 'angle_to_goal', 'goal_open_angle',\n",
        "    'dist_to_own_goal',\n",
        "\n",
        "    # í•„ë“œ ì˜ì—­\n",
        "    'is_left_side', 'is_center', 'is_right_side',\n",
        "    'is_final_third', 'is_near_touchline', 'min_dist_to_touchline',\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ í†µê³„ ìš”ì•½ (total_distance ì œê±°)\n",
        "    'seq_length',\n",
        "    'mean_distance', 'std_distance',\n",
        "    'max_distance', 'min_distance',\n",
        "    'mean_angle', 'std_angle',\n",
        "    'forward_ratio', 'backward_ratio',\n",
        "    'net_x_movement', 'net_y_movement',\n",
        "\n",
        "    # ìµœê·¼ ì´ë™\n",
        "    'recent_dx_mean', 'recent_dy_mean', 'recent_dist_mean',\n",
        "\n",
        "    # Lag1 í”¼ì²˜\n",
        "    'lag1_dx', 'lag1_dy', 'lag1_dist', 'lag1_angle',\n",
        "\n",
        "    # ì„ ìˆ˜/íŒ€ ì»¨í…ìŠ¤íŠ¸\n",
        "    'role',\n",
        "    'player_avg_dist', 'player_avg_dx',\n",
        "    'team_avg_pass_dist', 'team_possession_ratio',\n",
        "    'is_home',\n",
        "    'is_set_piece',\n",
        "\n",
        "    # ë§¤ì¹˜ ì»¨í…ìŠ¤íŠ¸ (is_weekend ì œê±°)\n",
        "    'match_phase',\n",
        "    'time_delta', 'episode_time',\n",
        "    'match_hour', 'time_slot',\n",
        "    'current_team_rest', 'opp_team_rest', 'rest_diff',\n",
        "\n",
        "    # ê²½ê¸° ìƒí™©\n",
        "    'cumulative_score_diff', 'is_draw',\n",
        "]\n",
        "\n",
        "print(f\"âœ… LSTM: {LSTM_CONTINUOUS_DIM}D continuous (ì ìˆ˜ì°¨ ì¶”ê°€)\")\n",
        "print(f\"âœ… LGBM: {len(LGBM_FEATURES)} meta features (total_distance, is_weekend ì œê±°)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LSTM: 10D continuous (ì ìˆ˜ì°¨ ì¶”ê°€)\n",
            "âœ… LGBM: 48 meta features (total_distance, is_weekend ì œê±°)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfBBR5WnYw70",
        "outputId": "0c185950-2d6e-4d22-8dc5-9e6dfbe395d6"
      },
      "source": [
        "def convert_to_relative(episode_data, score_diffs):\n",
        "    \"\"\"\n",
        "    ì ˆëŒ€ì¢Œí‘œ â†’ ìƒëŒ€ì¢Œí‘œ + ê³¨ëŒ€/ì¤‘ì•™ì„ /ì ìˆ˜ì°¨ í”¼ì²˜ (LSTMìš©)\n",
        "\n",
        "    Input:\n",
        "        episode_data: shape (T, 3) = [x, y, time]\n",
        "        score_diffs: shape (T,) = ê° ì‹œì ì˜ ì ìˆ˜ì°¨\n",
        "    Output: shape (T, 10) = [dx, dy, dist, angle, time, dist_to_goal, angle_to_goal, dist_to_own_goal, dist_to_center, score_diff]\n",
        "    \"\"\"\n",
        "    T = len(episode_data)\n",
        "    result = np.zeros((T, 10), dtype=np.float32)\n",
        "\n",
        "    GOAL_X, GOAL_Y = 1.0, 0.5\n",
        "    OWN_GOAL_X, OWN_GOAL_Y = 0.0, 0.5\n",
        "    CENTER_X = 0.5\n",
        "\n",
        "    for i in range(T):\n",
        "        x, y, t = episode_data[i]\n",
        "        result[i, 5] = np.sqrt((x - GOAL_X)**2 + (y - GOAL_Y)**2)\n",
        "        result[i, 6] = np.arctan2(GOAL_Y - y, GOAL_X - x) / np.pi\n",
        "        result[i, 7] = np.sqrt((x - OWN_GOAL_X)**2 + (y - OWN_GOAL_Y)**2)\n",
        "        result[i, 8] = np.abs(x - CENTER_X)\n",
        "        result[i, 9] = score_diffs[i] / 5.0  # ì ìˆ˜ì°¨ ì •ê·œí™” (ë³´í†µ -5 ~ +5)\n",
        "        result[i, 4] = t\n",
        "\n",
        "    result[0, :2] = episode_data[0, :2]\n",
        "\n",
        "    for i in range(1, T):\n",
        "        dx = episode_data[i, 0] - episode_data[i-1, 0]\n",
        "        dy = episode_data[i, 1] - episode_data[i-1, 1]\n",
        "        result[i, 0] = dx\n",
        "        result[i, 1] = dy\n",
        "        result[i, 2] = np.sqrt(dx**2 + dy**2)\n",
        "        result[i, 3] = np.arctan2(dy, dx) / np.pi\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def compute_zone_id(x, y):\n",
        "    \"\"\"í•„ë“œë¥¼ 3x3 Zoneìœ¼ë¡œ ë¶„í•  (0-8)\"\"\"\n",
        "    x_zone = 0 if x < 1/3 else (1 if x < 2/3 else 2)\n",
        "    y_zone = 0 if y < 1/3 else (1 if y < 2/3 else 2)\n",
        "    return x_zone * 3 + y_zone\n",
        "\n",
        "\n",
        "def compute_lgbm_features(ex, ey, sx, sy, norm_times, curr_types, curr_results,\n",
        "                          last_x, last_y, type_map,\n",
        "                          role, player_avg_dist, player_avg_dx,\n",
        "                          team_avg_pass_dist, team_possession_ratio,\n",
        "                          is_home, match_phase,\n",
        "                          match_hour, time_slot,\n",
        "                          current_team_rest, opp_team_rest, rest_diff,\n",
        "                          cumulative_score_diff):\n",
        "    \"\"\"LGBMìš© ë©”íƒ€ í”¼ì²˜ ê³„ì‚° (total_distance, is_weekend ì œê±°)\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    features['last_x'] = last_x\n",
        "    features['last_y'] = last_y\n",
        "    features['zone_id'] = compute_zone_id(last_x, last_y)\n",
        "\n",
        "    # ê³¨ëŒ€ ê´€ë ¨\n",
        "    features['dist_to_goal'] = np.sqrt((last_x - 1.0)**2 + (last_y - 0.5)**2)\n",
        "    features['angle_to_goal'] = np.arctan2(0.5 - last_y, 1.0 - last_x)\n",
        "    features['dist_to_own_goal'] = np.sqrt(last_x**2 + (last_y - 0.5)**2)\n",
        "\n",
        "    Y_NEAR, Y_FAR = 30.34/68, 37.66/68\n",
        "    angle_near = np.arctan2(Y_NEAR - last_y, 1.0 - last_x)\n",
        "    angle_far = np.arctan2(Y_FAR - last_y, 1.0 - last_x)\n",
        "    features['goal_open_angle'] = np.abs(angle_far - angle_near)\n",
        "\n",
        "    # í•„ë“œ ì˜ì—­\n",
        "    features['is_left_side'] = int(last_y < 0.33)\n",
        "    features['is_center'] = int(0.33 <= last_y <= 0.66)\n",
        "    features['is_right_side'] = int(last_y > 0.66)\n",
        "    features['is_final_third'] = int(last_x > 70/105)\n",
        "    features['min_dist_to_touchline'] = min(last_y, 1.0 - last_y)\n",
        "    features['is_near_touchline'] = int(features['min_dist_to_touchline'] < 5/68)\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ í†µê³„ (total_distance ì œê±°)\n",
        "    features['seq_length'] = len(ex)\n",
        "\n",
        "    if len(ex) > 1:\n",
        "        dx = np.diff(ex)\n",
        "        dy = np.diff(ey)\n",
        "        distances = np.sqrt(dx**2 + dy**2)\n",
        "        angles = np.arctan2(dy, dx)\n",
        "\n",
        "        # total_distance ì œê±°ë¨\n",
        "        features['mean_distance'] = np.mean(distances)\n",
        "        features['std_distance'] = np.std(distances) if len(distances) > 1 else 0\n",
        "        features['max_distance'] = np.max(distances)\n",
        "        features['min_distance'] = np.min(distances)\n",
        "        features['mean_angle'] = np.mean(angles)\n",
        "        features['std_angle'] = np.std(angles) if len(angles) > 1 else 0\n",
        "        features['forward_ratio'] = np.mean(dx > 0)\n",
        "        features['backward_ratio'] = np.mean(dx < 0)\n",
        "        features['net_x_movement'] = ex[-1] - ex[0]\n",
        "        features['net_y_movement'] = ey[-1] - ey[0]\n",
        "\n",
        "        recent_n = min(3, len(dx))\n",
        "        features['recent_dx_mean'] = np.mean(dx[-recent_n:])\n",
        "        features['recent_dy_mean'] = np.mean(dy[-recent_n:])\n",
        "        features['recent_dist_mean'] = np.mean(distances[-recent_n:])\n",
        "\n",
        "        features['lag1_dx'] = dx[-1]\n",
        "        features['lag1_dy'] = dy[-1]\n",
        "        features['lag1_dist'] = distances[-1]\n",
        "        features['lag1_angle'] = angles[-1]\n",
        "    else:\n",
        "        for k in ['mean_distance', 'std_distance', 'max_distance',\n",
        "                  'min_distance', 'mean_angle', 'std_angle', 'forward_ratio',\n",
        "                  'backward_ratio', 'net_x_movement', 'net_y_movement',\n",
        "                  'recent_dx_mean', 'recent_dy_mean', 'recent_dist_mean',\n",
        "                  'lag1_dx', 'lag1_dy', 'lag1_dist', 'lag1_angle']:\n",
        "            features[k] = 0\n",
        "\n",
        "    # ì„ ìˆ˜/íŒ€ ì»¨í…ìŠ¤íŠ¸\n",
        "    features['role'] = role\n",
        "    features['player_avg_dist'] = player_avg_dist\n",
        "    features['player_avg_dx'] = player_avg_dx\n",
        "    features['team_avg_pass_dist'] = team_avg_pass_dist\n",
        "    features['team_possession_ratio'] = team_possession_ratio\n",
        "    features['is_home'] = is_home\n",
        "\n",
        "    sp_types = [type_map.get(k, -1) for k in ['Corner', 'Freekick', 'Penalty', 'Kick Off'] if k in type_map]\n",
        "    features['is_set_piece'] = int(any(t in sp_types for t in curr_types))\n",
        "\n",
        "    # ë§¤ì¹˜ ì»¨í…ìŠ¤íŠ¸ (is_weekend ì œê±°)\n",
        "    features['match_phase'] = match_phase\n",
        "    features['time_delta'] = norm_times[-1] - norm_times[-2] if len(norm_times) > 1 else 0\n",
        "    features['episode_time'] = norm_times[-1] - norm_times[0] if len(norm_times) > 1 else 0\n",
        "    features['match_hour'] = match_hour\n",
        "    features['time_slot'] = time_slot\n",
        "    features['current_team_rest'] = current_team_rest\n",
        "    features['opp_team_rest'] = opp_team_rest\n",
        "    features['rest_diff'] = rest_diff\n",
        "\n",
        "    features['cumulative_score_diff'] = cumulative_score_diff\n",
        "    features['is_draw'] = int(cumulative_score_diff == 0)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"âœ… í”¼ì²˜ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (10D LSTM + ì ìˆ˜ì°¨)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… í”¼ì²˜ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (10D LSTM + ì ìˆ˜ì°¨)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFQoa2tVYw70"
      },
      "source": [
        "---\n",
        "## 06. ì‹œí€€ìŠ¤ ë¹Œë”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9kZFky8Yw70",
        "outputId": "18573853-4366-434b-e0db-c27b9ff8f767"
      },
      "source": [
        "def calculate_score_diff(episode_df, team_id, type_map):\n",
        "    \"\"\"\n",
        "    ì—í”¼ì†Œë“œ ë‚´ ì‹¤ì‹œê°„ ì ìˆ˜ì°¨ ê³„ì‚°\n",
        "    Goal ì´ë²¤íŠ¸ë¥¼ ì¶”ì í•˜ì—¬ ëˆ„ì  ì ìˆ˜ì°¨ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    score_diffs = []\n",
        "    our_score = 0\n",
        "    opp_score = 0\n",
        "\n",
        "    goal_type_id = type_map.get('Goal', -1)\n",
        "\n",
        "    for _, row in episode_df.iterrows():\n",
        "        # Goal ì´ë²¤íŠ¸ í™•ì¸\n",
        "        if row['type_name'] == 'Goal':\n",
        "            if row['team_id'] == team_id:\n",
        "                our_score += 1\n",
        "            else:\n",
        "                opp_score += 1\n",
        "\n",
        "        score_diffs.append(our_score - opp_score)\n",
        "\n",
        "    return np.array(score_diffs)\n",
        "\n",
        "\n",
        "def build_train_sequences(df, type_map, result_map, match_context, role_map, player_stats_map, team_stats_map):\n",
        "    \"\"\"\n",
        "    Train ì—í”¼ì†Œë“œë³„ ì‹œí€€ìŠ¤ êµ¬ì¶• (v9.2: ì ìˆ˜ì°¨ ì¶”ê°€)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    mc_dict = match_context.set_index('game_id').to_dict('index')\n",
        "    grouped = df.groupby('game_episode')\n",
        "\n",
        "    for episode_id, g in tqdm(grouped, desc=\"Train ì‹œí€€ìŠ¤ êµ¬ì¶•\"):\n",
        "        g = g.sort_values('time_seconds').reset_index(drop=True)\n",
        "\n",
        "        if len(g) < 2:\n",
        "            continue\n",
        "\n",
        "        # ì •ê·œí™” ì¢Œí‘œ\n",
        "        sx = g['start_x'].values / 105.0\n",
        "        sy = g['start_y'].values / 68.0\n",
        "        ex = g['end_x'].values / 105.0\n",
        "        ey = g['end_y'].values / 68.0\n",
        "\n",
        "        max_time = g['time_seconds'].max()\n",
        "        norm_times = g['time_seconds'].values / max_time if max_time > 0 else np.zeros(len(g))\n",
        "\n",
        "        curr_types = g['type_name'].map(type_map).fillna(0).values.astype(int)\n",
        "        curr_results = g['result_name'].fillna(\"Unknown\").map(result_map).fillna(0).values.astype(int)\n",
        "\n",
        "        # íŒ€ ì •ë³´\n",
        "        team_id = g['team_id'].iloc[-1]\n",
        "\n",
        "        # ì ìˆ˜ì°¨ ê³„ì‚° (NEW)\n",
        "        score_diffs = calculate_score_diff(g, team_id, type_map)\n",
        "\n",
        "        # Target: ë§ˆì§€ë§‰ end ì¢Œí‘œ\n",
        "        target_x, target_y = ex[-1], ey[-1]\n",
        "\n",
        "        # Input: ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì œì™¸\n",
        "        input_ex, input_ey = ex[:-1], ey[:-1]\n",
        "        input_sx, input_sy = sx[:-1], sy[:-1]\n",
        "        input_times = norm_times[:-1]\n",
        "        input_types = curr_types[:-1]\n",
        "        input_results = curr_results[:-1]\n",
        "        input_score_diffs = score_diffs[:-1]  # ì ìˆ˜ì°¨ë„ ë§ˆì§€ë§‰ ì œì™¸\n",
        "\n",
        "        if len(input_ex) < 1:\n",
        "            continue\n",
        "\n",
        "        last_x, last_y = input_ex[-1], input_ey[-1]\n",
        "\n",
        "        # ========== LSTMìš© ì‹œí€€ìŠ¤ (ì ìˆ˜ì°¨ í¬í•¨) ==========\n",
        "        abs_coords = np.column_stack([input_ex, input_ey, input_times])\n",
        "        continuous = convert_to_relative(abs_coords, input_score_diffs)\n",
        "\n",
        "        # ========== LGBMìš© ë©”íƒ€ í”¼ì²˜ ==========\n",
        "        game_id = g['game_id'].iloc[0]\n",
        "        mc = mc_dict.get(game_id, {})\n",
        "\n",
        "        player_id = g['player_id'].iloc[-2] if len(g) > 1 else g['player_id'].iloc[0]\n",
        "        role = role_map.get(player_id, 0)\n",
        "        p_stats = player_stats_map.get(player_id, {'player_avg_dist': 0, 'player_avg_dx': 0})\n",
        "        t_stats = team_stats_map.get(team_id, {'team_avg_pass_dist': 0, 'team_possession_ratio': 0})\n",
        "\n",
        "        is_home = int(team_id == mc.get('home_team_id', -1))\n",
        "        match_phase = int(g['time_seconds'].iloc[-1] // 900)\n",
        "        cumulative_score_diff = input_score_diffs[-1] if len(input_score_diffs) > 0 else 0\n",
        "\n",
        "        meta = compute_lgbm_features(\n",
        "            input_ex, input_ey, input_sx, input_sy,\n",
        "            input_times, input_types, input_results,\n",
        "            last_x, last_y, type_map,\n",
        "            role=role,\n",
        "            player_avg_dist=p_stats.get('player_avg_dist', 0),\n",
        "            player_avg_dx=p_stats.get('player_avg_dx', 0),\n",
        "            team_avg_pass_dist=t_stats.get('team_avg_pass_dist', 0),\n",
        "            team_possession_ratio=t_stats.get('team_possession_ratio', 0),\n",
        "            is_home=is_home,\n",
        "            match_phase=match_phase,\n",
        "            match_hour=mc.get('match_hour', 19),\n",
        "            time_slot=mc.get('time_slot', 1),\n",
        "            current_team_rest=mc.get('home_rest', 7) if is_home else mc.get('away_rest', 7),\n",
        "            opp_team_rest=mc.get('away_rest', 7) if is_home else mc.get('home_rest', 7),\n",
        "            rest_diff=mc.get('rest_diff', 0) if is_home else -mc.get('rest_diff', 0),\n",
        "            cumulative_score_diff=cumulative_score_diff\n",
        "        )\n",
        "\n",
        "        # Target: ìƒëŒ€ì¢Œí‘œ\n",
        "        target_dx = target_x - last_x\n",
        "        target_dy = target_y - last_y\n",
        "\n",
        "        sequences.append({\n",
        "            'continuous': continuous.astype(np.float32),\n",
        "            'types': input_types,\n",
        "            'results': input_results,\n",
        "            'target': np.array([target_dx, target_dy], dtype=np.float32),\n",
        "            'last_abs_coords': np.array([last_x, last_y], dtype=np.float32),\n",
        "            'episode_id': episode_id,\n",
        "            'meta_features': meta\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "print(\"âœ… build_train_sequences ì •ì˜ ì™„ë£Œ (ì ìˆ˜ì°¨ í¬í•¨)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… build_train_sequences ì •ì˜ ì™„ë£Œ (ì ìˆ˜ì°¨ í¬í•¨)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTlV4PDQYw71",
        "outputId": "b1487652-0761-4ea6-c6ca-5db60ead57dc"
      },
      "source": [
        "# ì‹œí€€ìŠ¤ êµ¬ì¶• (ì›ë³¸ ë°ì´í„°ë¡œ - ì¦ê°•ì€ Train/Val ë¶„í•  í›„ ì ìš©)\n",
        "print(\"ğŸ“¦ ì‹œí€€ìŠ¤ êµ¬ì¶• ì¤‘...\")\n",
        "sequences = build_train_sequences(\n",
        "    train_df, type_map, result_map, match_context,\n",
        "    role_map, player_stats_map, team_stats_map  # â† team_stats_map ì¶”ê°€\n",
        ")\n",
        "print(f\"âœ… ì´ {len(sequences):,}ê°œ ì‹œí€€ìŠ¤ (ì›ë³¸)\")\n",
        "\n",
        "# ë©”íƒ€ í”¼ì²˜ ê°œìˆ˜ í™•ì¸\n",
        "sample_meta = sequences[0]['meta_features']\n",
        "print(f\"âœ… Meta Features: {len(sample_meta)}ê°œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ ì‹œí€€ìŠ¤ êµ¬ì¶• ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train ì‹œí€€ìŠ¤ êµ¬ì¶•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15435/15435 [00:50<00:00, 308.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì´ 15,428ê°œ ì‹œí€€ìŠ¤ (ì›ë³¸)\n",
            "âœ… Meta Features: 48ê°œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_GHsJTjYw71"
      },
      "source": [
        "---\n",
        "## 07. LSTM ëª¨ë¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnwKMu8GYw71",
        "outputId": "9ad91b57-40f6-4ca7-dc4b-0fc37c858d30"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"LSTM Encoder with Simple Attention (v8.9 - MHA ì œê±°)\"\"\"\n",
        "\n",
        "    def __init__(self, num_types, num_results, embedding_dim=16, hidden_dim=128,\n",
        "                 num_layers=2, dropout=0.3, continuous_dim=9):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Embeddings\n",
        "        self.type_emb = nn.Embedding(num_types, embedding_dim)\n",
        "        self.result_emb = nn.Embedding(num_results, embedding_dim)\n",
        "\n",
        "        # LSTM\n",
        "        input_dim = continuous_dim + embedding_dim * 2\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Simple Attention\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Output head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, continuous, types, results, lengths, return_embedding=False):\n",
        "        batch_size = continuous.size(0)\n",
        "        max_len = continuous.size(1)\n",
        "        device = continuous.device\n",
        "\n",
        "        # Embedding concat\n",
        "        x = torch.cat([\n",
        "            continuous,\n",
        "            self.type_emb(types),\n",
        "            self.result_emb(results)\n",
        "        ], dim=-1)\n",
        "\n",
        "        # LSTM\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(packed)\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Padding mask\n",
        "        mask = torch.arange(max_len, device=device).expand(batch_size, max_len) >= lengths.to(device).unsqueeze(1)\n",
        "\n",
        "        # Simple Attention\n",
        "        attn_weights = self.attention(lstm_out).squeeze(-1)\n",
        "        attn_weights = attn_weights.masked_fill(mask, float('-inf'))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
        "\n",
        "        pred = self.head(context)\n",
        "\n",
        "        if return_embedding:\n",
        "            return pred, context\n",
        "        return pred\n",
        "\n",
        "    def extract_embedding(self, continuous, types, results, lengths):\n",
        "        with torch.no_grad():\n",
        "            _, emb = self.forward(continuous, types, results, lengths, return_embedding=True)\n",
        "        return emb\n",
        "\n",
        "\n",
        "print(\"âœ… LSTM + Simple Attention ëª¨ë¸ ì •ì˜ ì™„ë£Œ (v8.9 - MHA ì œê±°)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LSTM + Simple Attention ëª¨ë¸ ì •ì˜ ì™„ë£Œ (v8.9 - MHA ì œê±°)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1vvmD90Yw71"
      },
      "source": [
        "---\n",
        "## 08. Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsbLYBfYw71",
        "outputId": "4a74ec4e-b95c-4388-a320-fadb249cbe90"
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, is_train=True):\n",
        "        self.sequences = sequences\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        item = {\n",
        "            'continuous': torch.tensor(seq['continuous'], dtype=torch.float32),\n",
        "            'types': torch.tensor(seq['types'], dtype=torch.long),\n",
        "            'results': torch.tensor(seq['results'], dtype=torch.long),\n",
        "            'last_abs_coords': torch.tensor(seq['last_abs_coords'], dtype=torch.float32),\n",
        "            'meta_features': seq['meta_features']\n",
        "        }\n",
        "        if self.is_train:\n",
        "            item['target'] = torch.tensor(seq['target'], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    continuous = pad_sequence([b['continuous'] for b in batch], batch_first=True)\n",
        "    types = pad_sequence([b['types'] for b in batch], batch_first=True)\n",
        "    results = pad_sequence([b['results'] for b in batch], batch_first=True)\n",
        "    last_abs = torch.stack([b['last_abs_coords'] for b in batch])\n",
        "    lengths = torch.tensor([len(b['continuous']) for b in batch])\n",
        "\n",
        "    # Meta featuresë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ\n",
        "    meta_list = [b['meta_features'] for b in batch]\n",
        "\n",
        "    result = {\n",
        "        'continuous': continuous,\n",
        "        'types': types,\n",
        "        'results': results,\n",
        "        'lengths': lengths,\n",
        "        'last_abs_coords': last_abs,\n",
        "        'meta_features': meta_list\n",
        "    }\n",
        "\n",
        "    if 'target' in batch[0]:\n",
        "        result['target'] = torch.stack([b['target'] for b in batch])\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Dataset ì •ì˜ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2h6VdcWYw71"
      },
      "source": [
        "---\n",
        "## 09. LSTM í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xf9xlqiYw71",
        "outputId": "d58860d0-2ad4-4aef-e59e-d9585ff0e91b"
      },
      "source": [
        "# Train/Val ë¶„í•  (ì¦ê°• ì „!)\n",
        "train_seqs_raw, val_seqs = train_test_split(sequences, test_size=0.15, random_state=SEED)\n",
        "print(f\"ë¶„í•  í›„ - Train: {len(train_seqs_raw):,}, Val: {len(val_seqs):,}\")\n",
        "\n",
        "# Trainë§Œ ì¦ê°• (Valì€ ì›ë³¸ ìœ ì§€ â†’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "train_seqs = augment_y_flip_sequences(train_seqs_raw)\n",
        "print(f\"ì¦ê°• í›„ - Train: {len(train_seqs):,} (ì›ë³¸ì˜ 2ë°°), Val: {len(val_seqs):,} (ì›ë³¸ ìœ ì§€)\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    SequenceDataset(train_seqs, is_train=True),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    SequenceDataset(val_seqs, is_train=True),\n",
        "    batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"âœ… DataLoader ìƒì„± ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¶„í•  í›„ - Train: 13,113, Val: 2,315\n",
            "ì¦ê°• í›„ - Train: 26,226 (ì›ë³¸ì˜ 2ë°°), Val: 2,315 (ì›ë³¸ ìœ ì§€)\n",
            "âœ… DataLoader ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha4dWwZAYw71",
        "outputId": "e33aabed-afd7-4331-c4ca-b6eae09da395"
      },
      "source": [
        "# LSTM ëª¨ë¸ ìƒì„±\n",
        "lstm_model = LSTMEncoder(\n",
        "    NUM_TYPES, NUM_RESULTS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=LSTM_HIDDEN_DIM,\n",
        "    num_layers=LSTM_NUM_LAYERS,\n",
        "    dropout=LSTM_DROPOUT,\n",
        "    continuous_dim=LSTM_CONTINUOUS_DIM\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "print(f\"LSTM Parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Parameters: 638,131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQsoVEfTYw71",
        "outputId": "849f55bd-f2c4-46f4-ee80-6eb6a66ece4d"
      },
      "source": [
        "print(\"\\nğŸš€ LSTM í•™ìŠµ ì‹œì‘...\")\n",
        "best_val_dist = float('inf')\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, LSTM_EPOCHS + 1):\n",
        "    # Train\n",
        "    lstm_model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = lstm_model(\n",
        "            batch['continuous'].to(DEVICE),\n",
        "            batch['types'].to(DEVICE),\n",
        "            batch['results'].to(DEVICE),\n",
        "            batch['lengths']\n",
        "        )\n",
        "        loss = F.huber_loss(pred, batch['target'].to(DEVICE), delta=0.15)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    lstm_model.eval()\n",
        "    val_dists = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pred = lstm_model(\n",
        "                batch['continuous'].to(DEVICE),\n",
        "                batch['types'].to(DEVICE),\n",
        "                batch['results'].to(DEVICE),\n",
        "                batch['lengths']\n",
        "            )\n",
        "            pred = pred.cpu().numpy()\n",
        "            last_abs = batch['last_abs_coords'].numpy()\n",
        "            target = batch['target'].numpy()\n",
        "\n",
        "            pred_abs = (pred + last_abs) * [105, 68]\n",
        "            true_abs = (target + last_abs) * [105, 68]\n",
        "            dists = np.sqrt(np.sum((pred_abs - true_abs)**2, axis=1))\n",
        "            val_dists.extend(dists)\n",
        "\n",
        "    mean_dist = np.mean(val_dists)\n",
        "    scheduler.step(mean_dist)\n",
        "\n",
        "    if mean_dist < best_val_dist:\n",
        "        best_val_dist = mean_dist\n",
        "        best_state = {k: v.cpu().clone() for k, v in lstm_model.state_dict().items()}\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Val Distance = {mean_dist:.2f}m (best: {best_val_dist:.2f}m)\")\n",
        "\n",
        "# Best ëª¨ë¸ ë¡œë“œ\n",
        "lstm_model.load_state_dict(best_state)\n",
        "print(f\"\\nâœ… LSTM í•™ìŠµ ì™„ë£Œ: Best = {best_val_dist:.2f}m\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ LSTM í•™ìŠµ ì‹œì‘...\n",
            "Epoch 10: Val Distance = 15.54m (best: 15.50m)\n",
            "Epoch 20: Val Distance = 15.33m (best: 15.20m)\n",
            "Epoch 30: Val Distance = 15.16m (best: 15.08m)\n",
            "Epoch 40: Val Distance = 14.97m (best: 14.97m)\n",
            "Epoch 50: Val Distance = 14.93m (best: 14.93m)\n",
            "Epoch 60: Val Distance = 14.93m (best: 14.90m)\n",
            "Epoch 70: Val Distance = 14.89m (best: 14.87m)\n",
            "Epoch 80: Val Distance = 14.94m (best: 14.87m)\n",
            "Epoch 90: Val Distance = 14.92m (best: 14.87m)\n",
            "Epoch 100: Val Distance = 14.92m (best: 14.87m)\n",
            "\n",
            "âœ… LSTM í•™ìŠµ ì™„ë£Œ: Best = 14.87m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myff0Z9EYw71"
      },
      "source": [
        "---\n",
        "## 10. LGBM í”¼ì²˜ ì¶”ì¶œ ë° í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtT8zljeYw72",
        "outputId": "0a5bfa75-f33c-4dd2-902a-8540cf289e70"
      },
      "source": [
        "def extract_features_for_lgbm(sequences, lstm_model, device, is_train=True, batch_size=256):\n",
        "    \"\"\"LSTM ì„ë² ë”© + ë©”íƒ€ í”¼ì²˜ ê²°í•©\"\"\"\n",
        "    lstm_model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "    meta_feats = []\n",
        "    targets = []\n",
        "    last_abs_list = []\n",
        "\n",
        "    loader = DataLoader(\n",
        "        SequenceDataset(sequences, is_train=is_train),\n",
        "        batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # ë©”íƒ€ í”¼ì²˜ í‚¤ ì •ë ¬ (ì¼ê´€ì„±)\n",
        "    meta_keys = sorted(sequences[0]['meta_features'].keys())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Feature Extraction\"):\n",
        "            # LSTM ì„ë² ë”© ì¶”ì¶œ\n",
        "            emb = lstm_model.extract_embedding(\n",
        "                batch['continuous'].to(device),\n",
        "                batch['types'].to(device),\n",
        "                batch['results'].to(device),\n",
        "                batch['lengths']\n",
        "            )\n",
        "            embeddings.append(emb.cpu().numpy())\n",
        "            last_abs_list.append(batch['last_abs_coords'].numpy())\n",
        "\n",
        "            # ë©”íƒ€ í”¼ì²˜\n",
        "            for meta in batch['meta_features']:\n",
        "                meta_feats.append([meta.get(k, 0) for k in meta_keys])\n",
        "\n",
        "            if is_train:\n",
        "                targets.append(batch['target'].numpy())\n",
        "\n",
        "    # ê²°í•©\n",
        "    X_emb = np.vstack(embeddings)\n",
        "    X_meta = np.array(meta_feats)\n",
        "    X = np.hstack([X_emb, X_meta])\n",
        "    last_coords = np.vstack(last_abs_list)\n",
        "\n",
        "    feature_names = [f'lstm_emb_{i}' for i in range(X_emb.shape[1])] + meta_keys\n",
        "\n",
        "    if is_train:\n",
        "        return X, np.vstack(targets), last_coords, feature_names\n",
        "    return X, last_coords, feature_names\n",
        "\n",
        "print(\"âœ… LGBM í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LGBM í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnuKPqm6Yw72",
        "outputId": "8bd492db-f94e-412c-fade-95b5d2516de1"
      },
      "source": [
        "# í”¼ì²˜ ì¶”ì¶œ\n",
        "print(\"\\nğŸ“Š LGBM í”¼ì²˜ ì¶”ì¶œ...\")\n",
        "X_train, y_train, last_abs_train, feature_names = extract_features_for_lgbm(\n",
        "    train_seqs, lstm_model, DEVICE, is_train=True\n",
        ")\n",
        "X_val, y_val, last_abs_val, _ = extract_features_for_lgbm(\n",
        "    val_seqs, lstm_model, DEVICE, is_train=True\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"Features: {len(feature_names)}ê°œ (LSTM: {LSTM_HIDDEN_DIM*2}, Meta: {len(feature_names) - LSTM_HIDDEN_DIM*2})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š LGBM í”¼ì²˜ ì¶”ì¶œ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Feature Extraction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:02<00:00, 36.05it/s]\n",
            "Feature Extraction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 37.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (26226, 304)\n",
            "X_val shape: (2315, 304)\n",
            "Features: 304ê°œ (LSTM: 256, Meta: 48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2xdcg19Yw72",
        "outputId": "d7e1e93a-f1c7-47c1-8942-d6c1807ff08b"
      },
      "source": [
        "# LGBM í•™ìŠµ\n",
        "print(\"\\nğŸŒ² LightGBM í•™ìŠµ...\")\n",
        "\n",
        "lgbm_x = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
        "lgbm_y = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
        "\n",
        "lgbm_x.fit(\n",
        "    X_train, y_train[:, 0],\n",
        "    eval_set=[(X_val, y_val[:, 0])],\n",
        "    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
        ")\n",
        "\n",
        "lgbm_y.fit(\n",
        "    X_train, y_train[:, 1],\n",
        "    eval_set=[(X_val, y_val[:, 1])],\n",
        "    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
        ")\n",
        "\n",
        "print(f\"âœ… LGBM X: {lgbm_x.best_iteration_} iterations\")\n",
        "print(f\"âœ… LGBM Y: {lgbm_y.best_iteration_} iterations\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸŒ² LightGBM í•™ìŠµ...\n",
            "âœ… LGBM X: 231 iterations\n",
            "âœ… LGBM Y: 176 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fHC1wgYw72"
      },
      "source": [
        "---\n",
        "## 11. Validation í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv26wXY7Yw72",
        "outputId": "eee24355-2fb9-4138-fa7b-056413a93276"
      },
      "source": [
        "# ì˜ˆì¸¡\n",
        "pred_x = lgbm_x.predict(X_val)\n",
        "pred_y = lgbm_y.predict(X_val)\n",
        "\n",
        "# ì ˆëŒ€ì¢Œí‘œ ë³µì›\n",
        "pred_abs_x = np.clip((pred_x + last_abs_val[:, 0]) * 105.0, 0, 105)\n",
        "pred_abs_y = np.clip((pred_y + last_abs_val[:, 1]) * 68.0, 0, 68)\n",
        "\n",
        "true_abs_x = (y_val[:, 0] + last_abs_val[:, 0]) * 105.0\n",
        "true_abs_y = (y_val[:, 1] + last_abs_val[:, 1]) * 68.0\n",
        "\n",
        "# ê±°ë¦¬ ê³„ì‚°\n",
        "distances = np.sqrt((pred_abs_x - true_abs_x)**2 + (pred_abs_y - true_abs_y)**2)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“Š Validation Results (Hybrid LSTM + LGBM)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Mean Distance Error: {np.mean(distances):.2f}m\")\n",
        "print(f\"Median Distance Error: {np.median(distances):.2f}m\")\n",
        "print(f\"Std Distance Error: {np.std(distances):.2f}m\")\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ğŸ“Š Validation Results (Hybrid LSTM + LGBM)\n",
            "==================================================\n",
            "Mean Distance Error: 14.47m\n",
            "Median Distance Error: 11.91m\n",
            "Std Distance Error: 10.86m\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rrk0CFpYw72"
      },
      "source": [
        "---\n",
        "## 12. Feature Importance ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTTyNYVXYw72"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature Importance\n",
        "importance_x = lgbm_x.feature_importances_\n",
        "importance_y = lgbm_y.feature_importances_\n",
        "avg_importance = (importance_x + importance_y) / 2\n",
        "\n",
        "# Top 30\n",
        "top_idx = np.argsort(avg_importance)[-30:][::-1]\n",
        "top_features = [feature_names[i] for i in top_idx]\n",
        "top_importance = avg_importance[top_idx]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.barh(range(len(top_features)), top_importance[::-1])\n",
        "plt.yticks(range(len(top_features)), top_features[::-1])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 30 Feature Importance (LSTM Embedding + Meta Features)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# LSTM vs Meta ë¹„ì¤‘\n",
        "lstm_imp = np.sum(avg_importance[:LSTM_HIDDEN_DIM*2])\n",
        "meta_imp = np.sum(avg_importance[LSTM_HIDDEN_DIM*2:])\n",
        "print(f\"\\nğŸ“Š Feature Group Importance:\")\n",
        "print(f\"   LSTM Embedding: {lstm_imp:.1f} ({lstm_imp/(lstm_imp+meta_imp)*100:.1f}%)\")\n",
        "print(f\"   Meta Features:  {meta_imp:.1f} ({meta_imp/(lstm_imp+meta_imp)*100:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfaqOpnYw72"
      },
      "source": [
        "---\n",
        "## 13. ëª¨ë¸ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU--5yYvYw72"
      },
      "source": [
        "import pickle\n",
        "\n",
        "SAVE_DIR = f\"{BASE_DIR}/models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# LSTM ì €ì¥\n",
        "torch.save({\n",
        "    'model_state_dict': lstm_model.state_dict(),\n",
        "    'num_types': NUM_TYPES,\n",
        "    'num_results': NUM_RESULTS,\n",
        "    'hidden_dim': LSTM_HIDDEN_DIM,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "}, f\"{SAVE_DIR}/lstm_encoder.pt\")\n",
        "print(\"âœ… LSTM ì €ì¥\")\n",
        "\n",
        "# LGBM ì €ì¥\n",
        "lgbm_x.booster_.save_model(f\"{SAVE_DIR}/lgbm_x.txt\")\n",
        "lgbm_y.booster_.save_model(f\"{SAVE_DIR}/lgbm_y.txt\")\n",
        "print(\"âœ… LGBM ì €ì¥\")\n",
        "\n",
        "# ë§¤í•‘ ì €ì¥\n",
        "with open(f\"{SAVE_DIR}/mappings.pkl\", 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'type_map': type_map,\n",
        "        'result_map': result_map,\n",
        "        'role_map': role_map,\n",
        "        'player_stats_map': player_stats_map,\n",
        "        'feature_names': feature_names\n",
        "    }, f)\n",
        "print(\"âœ… ë§¤í•‘ ì €ì¥ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14. EDA"
      ],
      "metadata": {
        "id": "U4KmvyIVgSiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (Colab)\n",
        "!apt-get install -y fonts-nanum > /dev/null 2>&1\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "if os.path.exists(font_path):\n",
        "    font_name = fm.FontProperties(fname=font_path).get_name()\n",
        "    fm.fontManager.addfont(font_path)\n",
        "    plt.rcParams['font.family'] = font_name\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"âœ… ì‹œê°í™” ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "cnneNQ68gQ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# íˆìŠ¤í† ê·¸ë¨\n",
        "axes[0].hist(distances, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0].axvline(np.mean(distances), color='red', linestyle='--', label=f'Mean: {np.mean(distances):.2f}m')\n",
        "axes[0].axvline(np.median(distances), color='orange', linestyle='--', label=f'Median: {np.median(distances):.2f}m')\n",
        "axes[0].set_xlabel('Distance Error (m)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Error Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(distances, vert=True)\n",
        "axes[1].set_ylabel('Distance Error (m)')\n",
        "axes[1].set_title('Error Box Plot')\n",
        "\n",
        "# CDF\n",
        "sorted_dist = np.sort(distances)\n",
        "cdf = np.arange(1, len(sorted_dist) + 1) / len(sorted_dist)\n",
        "axes[2].plot(sorted_dist, cdf, color='steelblue')\n",
        "axes[2].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[2].axhline(0.9, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[2].set_xlabel('Distance Error (m)')\n",
        "axes[2].set_ylabel('Cumulative Probability')\n",
        "axes[2].set_title('Cumulative Distribution')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# í†µê³„ ì¶œë ¥\n",
        "print(\"\\nğŸ“Š ì˜¤ì°¨ í†µê³„:\")\n",
        "print(f\"  Mean:   {np.mean(distances):.2f}m\")\n",
        "print(f\"  Median: {np.median(distances):.2f}m\")\n",
        "print(f\"  Std:    {np.std(distances):.2f}m\")\n",
        "print(f\"  Min:    {np.min(distances):.2f}m\")\n",
        "print(f\"  Max:    {np.max(distances):.2f}m\")\n",
        "print(f\"  Q25:    {np.percentile(distances, 25):.2f}m\")\n",
        "print(f\"  Q75:    {np.percentile(distances, 75):.2f}m\")\n",
        "print(f\"  Q90:    {np.percentile(distances, 90):.2f}m\")"
      ],
      "metadata": {
        "id": "6Jd2cEPggWlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zone ë³„ ì˜¤ì°¨ ë¶„ì„"
      ],
      "metadata": {
        "id": "Ms9eKuKxgZ-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zoneë³„ ì˜¤ì°¨ ê³„ì‚°\n",
        "zone_errors = {i: [] for i in range(9)}\n",
        "zone_names = ['DEF_LEFT', 'DEF_CENTER', 'DEF_RIGHT',\n",
        "              'MID_LEFT', 'MID_CENTER', 'MID_RIGHT',\n",
        "              'ATT_LEFT', 'ATT_CENTER', 'ATT_RIGHT']\n",
        "\n",
        "for i, seq in enumerate(val_seqs):\n",
        "    zone_id = seq['meta_features'].get('zone_id', 4)\n",
        "    zone_id = min(max(int(zone_id), 0), 8)\n",
        "    zone_errors[zone_id].append(distances[i])\n",
        "\n",
        "# Zoneë³„ í†µê³„\n",
        "zone_stats = []\n",
        "for z in range(9):\n",
        "    if zone_errors[z]:\n",
        "        zone_stats.append({\n",
        "            'zone': zone_names[z],\n",
        "            'count': len(zone_errors[z]),\n",
        "            'mean': np.mean(zone_errors[z]),\n",
        "            'median': np.median(zone_errors[z]),\n",
        "            'std': np.std(zone_errors[z])\n",
        "        })\n",
        "\n",
        "zone_df = pd.DataFrame(zone_stats)\n",
        "print(\"ğŸ“Š Zoneë³„ ì˜¤ì°¨:\")\n",
        "print(zone_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "UxvTBzAzgX-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zone íˆíŠ¸ë§µ\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Mean Error íˆíŠ¸ë§µ\n",
        "mean_grid = np.zeros((3, 3))\n",
        "count_grid = np.zeros((3, 3))\n",
        "for z in range(9):\n",
        "    row, col = z // 3, z % 3\n",
        "    if zone_errors[z]:\n",
        "        mean_grid[row, col] = np.mean(zone_errors[z])\n",
        "        count_grid[row, col] = len(zone_errors[z])\n",
        "\n",
        "im1 = axes[0].imshow(mean_grid, cmap='RdYlGn_r', aspect='auto')\n",
        "axes[0].set_xticks([0, 1, 2])\n",
        "axes[0].set_yticks([0, 1, 2])\n",
        "axes[0].set_xticklabels(['Left', 'Center', 'Right'])\n",
        "axes[0].set_yticklabels(['Defense', 'Midfield', 'Attack'])\n",
        "axes[0].set_title('Mean Distance Error by Zone (m)')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        axes[0].text(j, i, f'{mean_grid[i, j]:.1f}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# Sample Count íˆíŠ¸ë§µ\n",
        "im2 = axes[1].imshow(count_grid, cmap='Blues', aspect='auto')\n",
        "axes[1].set_xticks([0, 1, 2])\n",
        "axes[1].set_yticks([0, 1, 2])\n",
        "axes[1].set_xticklabels(['Left', 'Center', 'Right'])\n",
        "axes[1].set_yticklabels(['Defense', 'Midfield', 'Attack'])\n",
        "axes[1].set_title('Sample Count by Zone')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        axes[1].text(j, i, f'{int(count_grid[i, j])}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RprJMHX6ghRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "importance_x = lgbm_x.feature_importances_\n",
        "importance_y = lgbm_y.feature_importances_\n",
        "avg_importance = (importance_x + importance_y) / 2\n",
        "\n",
        "# LSTM vs Meta ë¶„ë¦¬\n",
        "lstm_dim = LSTM_HIDDEN_DIM * 2  # Bidirectional\n",
        "lstm_importance = avg_importance[:lstm_dim]\n",
        "meta_importance = avg_importance[lstm_dim:]\n",
        "meta_names = feature_names[lstm_dim:]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Meta Features Top 25\n",
        "top_meta_idx = np.argsort(meta_importance)[-25:][::-1]\n",
        "top_meta_names = [meta_names[i] for i in top_meta_idx]\n",
        "top_meta_values = meta_importance[top_meta_idx]\n",
        "\n",
        "axes[0].barh(range(len(top_meta_names)), top_meta_values[::-1], color='coral')\n",
        "axes[0].set_yticks(range(len(top_meta_names)))\n",
        "axes[0].set_yticklabels(top_meta_names[::-1])\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Top 25 Meta Features')\n",
        "\n",
        "# LSTM Embedding Importance ë¶„í¬\n",
        "axes[1].hist(lstm_importance, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[1].axvline(np.mean(lstm_importance), color='red', linestyle='--',\n",
        "                label=f'Mean: {np.mean(lstm_importance):.1f}')\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_title(f'LSTM Embedding Importance Distribution ({lstm_dim}D)')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ë¹„ì¤‘ ì¶œë ¥\n",
        "lstm_total = np.sum(lstm_importance)\n",
        "meta_total = np.sum(meta_importance)\n",
        "print(f\"\\nğŸ“Š Feature Group Contribution:\")\n",
        "print(f\"  LSTM Embedding ({lstm_dim}D): {lstm_total:.1f} ({lstm_total/(lstm_total+meta_total)*100:.1f}%)\")\n",
        "print(f\"  Meta Features ({len(meta_names)}D):  {meta_total:.1f} ({meta_total/(lstm_total+meta_total)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "xELqqQ11gmV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡ê°’ ê³„ì‚° (ì´ë¯¸ ìœ„ì—ì„œ í–ˆë‹¤ë©´ ìƒëµ)\n",
        "pred_x = lgbm_x.predict(X_val)\n",
        "pred_y = lgbm_y.predict(X_val)\n",
        "pred_abs_x = np.clip((pred_x + last_abs_val[:, 0]) * 105.0, 0, 105)\n",
        "pred_abs_y = np.clip((pred_y + last_abs_val[:, 1]) * 68.0, 0, 68)\n",
        "true_abs_x = (y_val[:, 0] + last_abs_val[:, 0]) * 105.0\n",
        "true_abs_y = (y_val[:, 1] + last_abs_val[:, 1]) * 68.0\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# X ì¢Œí‘œ\n",
        "axes[0].scatter(true_abs_x, pred_abs_x, alpha=0.2, s=5)\n",
        "axes[0].plot([0, 105], [0, 105], 'r--', label='Perfect')\n",
        "axes[0].set_xlabel('True X (m)')\n",
        "axes[0].set_ylabel('Predicted X (m)')\n",
        "axes[0].set_title('X Coordinate: Predicted vs True')\n",
        "axes[0].legend()\n",
        "axes[0].set_xlim(0, 105)\n",
        "axes[0].set_ylim(0, 105)\n",
        "\n",
        "# Y ì¢Œí‘œ\n",
        "axes[1].scatter(true_abs_y, pred_abs_y, alpha=0.2, s=5)\n",
        "axes[1].plot([0, 68], [0, 68], 'r--', label='Perfect')\n",
        "axes[1].set_xlabel('True Y (m)')\n",
        "axes[1].set_ylabel('Predicted Y (m)')\n",
        "axes[1].set_title('Y Coordinate: Predicted vs True')\n",
        "axes[1].legend()\n",
        "axes[1].set_xlim(0, 68)\n",
        "axes[1].set_ylim(0, 68)\n",
        "\n",
        "# í•„ë“œ ìœ„ ë¶„í¬ (ìƒ˜í”Œë§)\n",
        "sample_idx = np.random.choice(len(pred_abs_x), min(500, len(pred_abs_x)), replace=False)\n",
        "axes[2].scatter(true_abs_x[sample_idx], true_abs_y[sample_idx],\n",
        "                alpha=0.5, s=20, c='blue', label='True', marker='o')\n",
        "axes[2].scatter(pred_abs_x[sample_idx], pred_abs_y[sample_idx],\n",
        "                alpha=0.5, s=20, c='red', label='Predicted', marker='x')\n",
        "# í•„ë“œ ë¼ì¸\n",
        "axes[2].axvline(52.5, color='gray', linestyle='-', alpha=0.3)\n",
        "axes[2].axhline(34, color='gray', linestyle='-', alpha=0.3)\n",
        "axes[2].set_xlim(0, 105)\n",
        "axes[2].set_ylim(0, 68)\n",
        "axes[2].set_xlabel('X (m)')\n",
        "axes[2].set_ylabel('Y (m)')\n",
        "axes[2].set_title('Field Distribution (500 samples)')\n",
        "axes[2].legend()\n",
        "axes[2].set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "preRw_H-g6bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Test ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "\n",
        "1.   í•­ëª© ì¶”ê°€\n",
        "2.   í•­ëª© ì¶”ê°€\n",
        "\n"
      ],
      "metadata": {
        "id": "jD6ppbE5hkaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_test_sequences(test_meta, base_dir, type_map, result_map, match_context,\n",
        "                         role_map, player_stats_map, team_stats_map):\n",
        "    \"\"\"Test ì‹œí€€ìŠ¤ êµ¬ì¶• (v9.2: ì ìˆ˜ì°¨ í¬í•¨)\"\"\"\n",
        "    sequences = []\n",
        "    episode_ids = []\n",
        "    mc_dict = match_context.set_index('game_id').to_dict('index')\n",
        "\n",
        "    for _, row in tqdm(test_meta.iterrows(), total=len(test_meta), desc=\"Test ì‹œí€€ìŠ¤ êµ¬ì¶•\"):\n",
        "        episode_id = row['game_episode']\n",
        "        game_id = row['game_id']\n",
        "\n",
        "        episode_df = pd.read_csv(os.path.join(base_dir, row['path']))\n",
        "        episode_df = episode_df.sort_values('time_seconds').reset_index(drop=True)\n",
        "\n",
        "        if pd.isna(episode_df.iloc[-1]['end_x']):\n",
        "            input_df = episode_df.iloc[:-1].copy()\n",
        "        else:\n",
        "            input_df = episode_df.copy()\n",
        "\n",
        "        if len(input_df) < 1:\n",
        "            last_x = episode_df.iloc[0]['start_x'] / 105.0\n",
        "            last_y = episode_df.iloc[0]['start_y'] / 68.0\n",
        "            sequences.append({\n",
        "                'continuous': np.zeros((1, 10), dtype=np.float32),  # 10D\n",
        "                'types': np.array([0]),\n",
        "                'results': np.array([0]),\n",
        "                'last_abs_coords': np.array([last_x, last_y], dtype=np.float32),\n",
        "                'meta_features': {k: 0 for k in sorted(LGBM_FEATURES)}\n",
        "            })\n",
        "            episode_ids.append(episode_id)\n",
        "            continue\n",
        "\n",
        "        sx = input_df['start_x'].values / 105.0\n",
        "        sy = input_df['start_y'].values / 68.0\n",
        "        ex = input_df['end_x'].values / 105.0\n",
        "        ey = input_df['end_y'].values / 68.0\n",
        "\n",
        "        max_time = input_df['time_seconds'].max()\n",
        "        norm_times = input_df['time_seconds'].values / max_time if max_time > 0 else np.zeros(len(input_df))\n",
        "\n",
        "        curr_types = input_df['type_name'].map(type_map).fillna(0).values.astype(int)\n",
        "        curr_results = input_df['result_name'].fillna(\"Unknown\").map(result_map).fillna(0).values.astype(int)\n",
        "\n",
        "        # íŒ€ ì •ë³´ & ì ìˆ˜ì°¨ ê³„ì‚°\n",
        "        team_id = input_df['team_id'].iloc[-1]\n",
        "        score_diffs = calculate_score_diff(input_df, team_id, type_map)\n",
        "\n",
        "        last_x, last_y = ex[-1], ey[-1]\n",
        "\n",
        "        abs_coords = np.column_stack([ex, ey, norm_times])\n",
        "        continuous = convert_to_relative(abs_coords, score_diffs)\n",
        "\n",
        "        mc = mc_dict.get(game_id, {})\n",
        "        player_id = input_df['player_id'].iloc[-1]\n",
        "        role = role_map.get(player_id, 0)\n",
        "        p_stats = player_stats_map.get(player_id, {'player_avg_dist': 0, 'player_avg_dx': 0})\n",
        "        t_stats = team_stats_map.get(team_id, {'team_avg_pass_dist': 0, 'team_possession_ratio': 0})\n",
        "\n",
        "        is_home = int(team_id == mc.get('home_team_id', -1))\n",
        "        match_phase = int(input_df['time_seconds'].iloc[-1] // 900)\n",
        "        cumulative_score_diff = score_diffs[-1] if len(score_diffs) > 0 else 0\n",
        "\n",
        "        meta = compute_lgbm_features(\n",
        "            ex, ey, sx, sy, norm_times, curr_types, curr_results,\n",
        "            last_x, last_y, type_map,\n",
        "            role=role,\n",
        "            player_avg_dist=p_stats.get('player_avg_dist', 0),\n",
        "            player_avg_dx=p_stats.get('player_avg_dx', 0),\n",
        "            team_avg_pass_dist=t_stats.get('team_avg_pass_dist', 0),\n",
        "            team_possession_ratio=t_stats.get('team_possession_ratio', 0),\n",
        "            is_home=is_home,\n",
        "            match_phase=match_phase,\n",
        "            match_hour=mc.get('match_hour', 19),\n",
        "            time_slot=mc.get('time_slot', 1),\n",
        "            current_team_rest=mc.get('home_rest', 7) if is_home else mc.get('away_rest', 7),\n",
        "            opp_team_rest=mc.get('away_rest', 7) if is_home else mc.get('home_rest', 7),\n",
        "            rest_diff=mc.get('rest_diff', 0) if is_home else -mc.get('rest_diff', 0),\n",
        "            cumulative_score_diff=cumulative_score_diff\n",
        "        )\n",
        "\n",
        "        sequences.append({\n",
        "            'continuous': continuous.astype(np.float32),\n",
        "            'types': curr_types,\n",
        "            'results': curr_results,\n",
        "            'last_abs_coords': np.array([last_x, last_y], dtype=np.float32),\n",
        "            'meta_features': meta\n",
        "        })\n",
        "        episode_ids.append(episode_id)\n",
        "\n",
        "    return sequences, episode_ids\n",
        "\n",
        "print(\"âœ… build_test_sequences ì •ì˜ ì™„ë£Œ (ì ìˆ˜ì°¨ í¬í•¨)\")"
      ],
      "metadata": {
        "id": "Kh6XWLE1hZ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test ë°ì´í„° ë¡œë“œ ë° ì‹œí€€ìŠ¤ êµ¬ì¶•\n",
        "print(\"ğŸ“¦ Test ì‹œí€€ìŠ¤ êµ¬ì¶• ì¤‘...\")\n",
        "test_meta = pd.read_csv(TEST_PATH)\n",
        "print(f\"Test Episodes: {len(test_meta)}\")\n",
        "\n",
        "test_seqs, test_ids = build_test_sequences(\n",
        "    test_meta, BASE_DIR, type_map, result_map, match_context,\n",
        "    role_map, player_stats_map, team_stats_map  # â† team_stats_map ì¶”ê°€\n",
        ")\n",
        "print(f\"âœ… Test ì‹œí€€ìŠ¤: {len(test_seqs)}ê°œ\")"
      ],
      "metadata": {
        "id": "HkJmsUnVhnqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test í”¼ì²˜ ì¶”ì¶œ\n",
        "print(\"\\nğŸ“Š Test í”¼ì²˜ ì¶”ì¶œ...\")\n",
        "X_test, last_abs_test, _ = extract_features_for_lgbm(\n",
        "    test_seqs, lstm_model, DEVICE, is_train=False\n",
        ")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "W93VilZNho3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡\n",
        "print(\"\\nğŸ”® Test ì˜ˆì¸¡...\")\n",
        "test_pred_x = lgbm_x.predict(X_test)\n",
        "test_pred_y = lgbm_y.predict(X_test)\n",
        "\n",
        "# ì ˆëŒ€ì¢Œí‘œ ë³µì›\n",
        "test_abs_x = np.clip((test_pred_x + last_abs_test[:, 0]) * 105.0, 0, 105)\n",
        "test_abs_y = np.clip((test_pred_y + last_abs_test[:, 1]) * 68.0, 0, 68)\n",
        "\n",
        "print(f\"\\nğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
        "print(f\"  end_x: mean={np.mean(test_abs_x):.2f}, std={np.std(test_abs_x):.2f}, range=[{np.min(test_abs_x):.1f}, {np.max(test_abs_x):.1f}]\")\n",
        "print(f\"  end_y: mean={np.mean(test_abs_y):.2f}, std={np.std(test_abs_y):.2f}, range=[{np.min(test_abs_y):.1f}, {np.max(test_abs_y):.1f}]\")\n",
        "\n",
        "# ========== ì œì¶œ íŒŒì¼ ìƒì„± ==========\n",
        "print(\"\\nğŸ“ ì œì¶œ íŒŒì¼ ìƒì„±...\")\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'game_episode': test_ids,\n",
        "    'end_x': test_abs_x,\n",
        "    'end_y': test_abs_y\n",
        "})\n",
        "\n",
        "SUBMIT_PATH = f\"{BASE_DIR}/submission_v9_2_8_Hubor.csv\"\n",
        "submission_df.to_csv(SUBMIT_PATH, index=False)\n",
        "\n",
        "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {SUBMIT_PATH}\")\n",
        "print(f\"   ì´ {len(submission_df)}ê°œ ì˜ˆì¸¡\")\n",
        "\n",
        "print(\"\\nğŸ“‹ ì œì¶œ íŒŒì¼ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
        "print(submission_df.head().to_string(index=False))"
      ],
      "metadata": {
        "id": "9g5TkC-9hq6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KdL5Fw83D5s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}